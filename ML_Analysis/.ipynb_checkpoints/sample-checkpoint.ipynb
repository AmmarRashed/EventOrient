{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, nlargest\n",
    "import warnings\n",
    "import unicodedata, re\n",
    "try:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    usesklearn = True\n",
    "except ImportError:\n",
    "    usesklearn = False\n",
    "\n",
    "import warnings\n",
    "\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "from textblob import TextBlob\n",
    "from textblob.exceptions import NotTranslated\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, samples=5, keep_authors_status=False):\n",
    "        self.cluster_vector = None\n",
    "        self.vectors_sim = None  # [(similarity, np.zeros(vector_size))]\n",
    "        self.vectors = list()  # [list of cluster vectors]\n",
    "        self.authors = dict()\n",
    "        self.keep_authors_status = keep_authors_status\n",
    "        self.samples = [None for i in range(samples)]\n",
    "        self._nonesamples = samples-1\n",
    "\n",
    "    def root_similarity(self, v1):\n",
    "        \"\"\"\n",
    "            similarity_to_cluster_vector\n",
    "        :param v1: np 1-D array\n",
    "        :return: similarity to the cluster vector\n",
    "        \"\"\"\n",
    "        return self.cos_sim(v1, self.cluster_vector)\n",
    "\n",
    "    def get_top_n(self, n=5):\n",
    "        \"\"\"\n",
    "        :param n: number of most similar vectors\n",
    "        :return: most similar n vectors to that cluster\n",
    "        \"\"\"\n",
    "        ## TODO return items\n",
    "        if n > len(self.vectors):\n",
    "            warnings.warn(\"n is bigger than the number of vectors in that cluster\")\n",
    "        return nlargest(min(n, len(self.vectors)), self.vectors_sim)\n",
    "\n",
    "    @staticmethod\n",
    "    def cos_sim(v1, v2):\n",
    "        if usesklearn:\n",
    "            return np.float64(cosine_similarity(np.atleast_2d(v1),np.atleast_2d(v2)))\n",
    "        else:\n",
    "            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "    def update_cluster_vector(self):\n",
    "        self.vectors_sim = list()\n",
    "        self.cluster_vector = np.mean(np.array(self.vectors), axis=0)\n",
    "        for vector in self.vectors:\n",
    "            heappush(self.vectors_sim, (self.root_similarity(vector), vector))\n",
    "            # self.vectors_sim.append((self.root_similarity(vector), vector))\n",
    "        assert self.cluster_vector is not None\n",
    "    \n",
    "    def addtext(self, text, author):\n",
    "        if self.keep_authors_status and author:\n",
    "            self.authors.setdefault(author, list())\n",
    "            self.authors[author].append(text)\n",
    "        else:\n",
    "            self.authors.setdefault(author, 0)\n",
    "            self.authors[author] += 1\n",
    "        \n",
    "        if self._nonesamples >= 0:\n",
    "            self.samples[self._nonesamples] = text\n",
    "            self._nonesamples -= 1\n",
    "        elif np.random.choice([True, False]):\n",
    "            self.samples[np.random.randint(0, len(self.samples))] = text\n",
    "    \n",
    "    def add(self, vector, text, author):\n",
    "#         try:\n",
    "        self.vectors.append(vector)\n",
    "        self.addtext(text, author)\n",
    "        self.update_cluster_vector()\n",
    "#         except:\n",
    "#             warnings.warn(\"An error occured during updating the cluster metrics. Last changes reveresed.\")\n",
    "#             return True\n",
    "\n",
    "    def __hash__(self):\n",
    "        h = hash(str(self.cluster_vector))\n",
    "        for i in self.vectors[:min(len(self.vectors), 3)]:\n",
    "            h *= hash(str(i))\n",
    "        return h\n",
    "\n",
    "\n",
    "class AdaptiveOnlineClustering:\n",
    "\n",
    "    def __init__(self, en_w2v, tr_w2v, similarity_threshold=0.75, cluster_nsamples=5, vector_size=300):\n",
    "        self.en_w2v = en_w2v\n",
    "        self.tr_w2v = tr_w2v\n",
    "        self.vector_size = vector_size\n",
    "        self.turkish_stemmer = TurkishStemmer()\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.cluster_nsamples = cluster_nsamples\n",
    "        self.clusters = dict()\n",
    "\n",
    "    def add(self, text, language=\"en\", author=None, translate=True, stem=False):\n",
    "        vec = self.vectorize(text, language, translate=translate, stem=stem)\n",
    "        if vec is None:\n",
    "            warnings.warn(\"Invalid text. Document skipped\")\n",
    "        else:\n",
    "            self._add(vec, text, author)\n",
    "\n",
    "    def _add(self, vector, text, author):\n",
    "        highest_similarity = 0\n",
    "        assigned_cluster = None\n",
    "        for cluster in self.clusters:\n",
    "            sim = self.clusters[cluster].root_similarity(vector)\n",
    "            if sim > highest_similarity:\n",
    "                highest_similarity = sim\n",
    "                assigned_cluster = cluster\n",
    "        if highest_similarity >= self.similarity_threshold:\n",
    "            self.clusters[assigned_cluster].add(vector, text, author)\n",
    "        else:\n",
    "            new_cluster = Cluster(self.cluster_nsamples)\n",
    "            added = new_cluster.add(vector, text, author)\n",
    "            self.clusters[len(self.clusters)] = new_cluster\n",
    "        self._update_clusters()\n",
    "\n",
    "    def _update_clusters(self):\n",
    "        for cluster in self.clusters:\n",
    "            if len(self.clusters[cluster].vectors) < 1:\n",
    "                del self.clusters[cluster]\n",
    "\n",
    "    def vectorize(self, text, language, translate=True, stem=False):\n",
    "        blob = self.clean(text, language, translate=translate, stem=stem)\n",
    "        if not blob:\n",
    "            return\n",
    "        vector = np.zeros(self.vector_size)\n",
    "        if len(blob.words) < 1:\n",
    "            return None\n",
    "\n",
    "        for word in blob.words:\n",
    "            try:\n",
    "                if language == \"en\" or translate:\n",
    "                    vector += self.en_w2v[word]\n",
    "                else:\n",
    "                    vector += self.tr_w2v[word]\n",
    "            except KeyError:\n",
    "                continue\n",
    "        vector /= len(blob.words)\n",
    "        return vector\n",
    "\n",
    "    def clean(self, text, language=\"en\", translate=True, stem=False):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').lower().decode(\"ascii\")\n",
    "        # if language == \"tr\":\n",
    "        #     if stem:\n",
    "        #         text= ' '.join([self.turkish_stemmer.stem(w) for w in text.split()])\n",
    "        blob = TextBlob(text)\n",
    "        if translate and language != \"en\":\n",
    "            try:\n",
    "                blob = blob.translate(to=\"en\")\n",
    "            except NotTranslated:\n",
    "                return \n",
    "        text = str(blob)\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = re.sub(r'[0-9]', '#', text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\"!\", \" \", text)\n",
    "        text = re.sub(r\"\\/\", \" \", text)\n",
    "        text = re.sub(r\"\\^\", \" \", text)\n",
    "        text = re.sub(r\"\\+\", \" \", text)\n",
    "        text = re.sub(r\"\\-\", \" \", text)\n",
    "        text = re.sub(r\"\\=\", \" \", text)\n",
    "        text = re.sub(r\"'\", \" \", text)\n",
    "        text = re.sub(r\":\", \" \", text)\n",
    "        text = re.sub(r\"e(\\s)?-(\\s)?mail\", \"email\", text)\n",
    "\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"cannot \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\" e g \", \" eg \", text)\n",
    "        text = re.sub(r\" b g \", \" bg \", text)\n",
    "        text = re.sub(r\" u s \", \" american \", text)\n",
    "        return TextBlob(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from AdaptiveOnlineClustering import AdaptiveOnlineClustering as aoc\n",
    "import gensim, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pickle.load(open('../datasets/tweets.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/ammar/NLP_data/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AdaptiveOnlineClustering(model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tweets[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927954050"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.author.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:105: UserWarning: Invalid text. Document skipped\n"
     ]
    }
   ],
   "source": [
    "for t in tweets[:5]:\n",
    "    for i in t[:5]:\n",
    "#         if i.lang == \"en\":\n",
    "#         print(i.author.id)\n",
    "        a.add(i.text, i.lang, i.author.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "Number of samples: 5\n",
      "Number of tweets: 7\n",
      "Number of authors: 4\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {927954050: 3, 3068366386: 2, 341498661: 1, 2386861894: 1}\n",
      "\n",
      "Cluster 1:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {927954050: 1}\n",
      "\n",
      "Cluster 2:\n",
      "Number of samples: 5\n",
      "Number of tweets: 5\n",
      "Number of authors: 3\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {3068366386: 1, 341498661: 1, 2386861894: 3}\n",
      "\n",
      "Cluster 3:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {341498661: 1}\n",
      "\n",
      "Cluster 4:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 5:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 6:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 7:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 8:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 9:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2386861894: 1}\n",
      "\n",
      "Cluster 10:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {3068366386: 1}\n",
      "\n",
      "Cluster 11:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {3068366386: 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for e,c in a.clusters.items():\n",
    "#     print(c.samples)\n",
    "    print(\"Cluster {}:\".format(e))\n",
    "    print(\"Number of samples:\",sum([i is not None for i in c.samples]))\n",
    "    print(\"Number of tweets:\",len(c.vectors))\n",
    "    print(\"Number of authors:\",len(c.authors))\n",
    "    print(\"Authors IDs > # of their tweets in that cluster: \\n\", c.authors)\n",
    "    print()\n",
    "#     print(c.samples[0] is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
