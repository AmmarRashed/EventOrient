{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, nlargest\n",
    "import warnings\n",
    "import unicodedata, re\n",
    "\n",
    "import warnings\n",
    "\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "from textblob import TextBlob\n",
    "from textblob.exceptions import NotTranslated\n",
    "\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self):\n",
    "        self.cluster_vector = None\n",
    "        self.vectors_sim = None  # [(similarity, np.zeros(vector_size))]\n",
    "        self.vectors = list()  # [list of cluster vectors]\n",
    "\n",
    "    def root_similarity(self, v1):\n",
    "        \"\"\"\n",
    "            similarity_to_cluster_vector\n",
    "        :param v1: np 1-D array\n",
    "        :return: similarity to the cluster vector\n",
    "        \"\"\"\n",
    "        return self.cos_sim(v1, self.cluster_vector)\n",
    "\n",
    "    def get_top_n(self, n):\n",
    "        \"\"\"\n",
    "        :param n: number of most similar vectors\n",
    "        :return: most similar n vectors to that cluster\n",
    "        \"\"\"\n",
    "        ## TODO return items\n",
    "        if n > len(self.vectors):\n",
    "            warnings.warn(\"n is bigger than the number of vectors in that cluster\")\n",
    "        return nlargest(min(n, len(self.vectors)), self.vectors_sim)\n",
    "\n",
    "    @staticmethod\n",
    "    def cos_sim(v1, v2):\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "    def update_cluster_vector(self):\n",
    "        self.vectors_sim = list()\n",
    "        self.cluster_vector = np.mean(np.array(self.vectors), axis=0)\n",
    "        for vector in self.vectors:\n",
    "            heappush(self.vectors_sim, (self.root_similarity(vector), vector))\n",
    "            # self.vectors_sim.append((self.root_similarity(vector), vector))\n",
    "\n",
    "    def add(self, vector):\n",
    "        try:\n",
    "            self.vectors.append(vector)\n",
    "            self.update_cluster_vector()\n",
    "        except:\n",
    "            warnings.warn(\"An error occured during updating the cluster metrics. Last changes reveresed.\")\n",
    "\n",
    "    def __hash__(self):\n",
    "        h = hash(str(self.cluster_vector))\n",
    "        for i in self.vectors[:min(len(self.vectors), 3)]:\n",
    "            h *= hash(str(i))\n",
    "        return h\n",
    "\n",
    "\n",
    "class AdaptiveOnlineClustering:\n",
    "\n",
    "    def __init__(self, en_w2v, tr_w2v, similarity_threshold=0.75, vector_size=300):\n",
    "        self.en_w2v = en_w2v\n",
    "        self.tr_w2v = tr_w2v\n",
    "        self.vector_size = vector_size\n",
    "        self.turkish_stemmer = TurkishStemmer()\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.clusters = dict()\n",
    "\n",
    "    def add(self, text, language=\"en\", translate=True, stem=False):\n",
    "        vec = self.vectorize(text, language, translate=translate, stem=stem)\n",
    "        if vec is None:\n",
    "            warnings.warn(\"Invalid text. Document skipped\")\n",
    "        else:\n",
    "            self.add_(vec)\n",
    "\n",
    "    def add_(self, vector):\n",
    "        highest_similarity = 0\n",
    "        assigned_cluster = None\n",
    "        for cluster in self.clusters:\n",
    "            sim = self.clusters[cluster].root_similarity(vector)\n",
    "            if sim > highest_similarity:\n",
    "                highest_similarity = sim\n",
    "                assigned_cluster = cluster\n",
    "        if highest_similarity >= self.similarity_threshold:\n",
    "            self.clusters[assigned_cluster].add(vector)\n",
    "        else:\n",
    "            new_cluster = Cluster()\n",
    "            new_cluster.add(vector)\n",
    "            self.clusters[len(self.clusters)] = new_cluster\n",
    "        self._update_clusters()\n",
    "\n",
    "    def _update_clusters(self):\n",
    "        for cluster in self.clusters:\n",
    "            if len(self.clusters[cluster].vectors) < 1:\n",
    "                del self.clusters[cluster]\n",
    "\n",
    "    def vectorize(self, text, language, translate=True, stem=False):\n",
    "        blob = self.clean(text, language, translate=translate, stem=stem)\n",
    "        if not blob:\n",
    "            return\n",
    "        vector = np.zeros(self.vector_size)\n",
    "        if len(blob.words) < 1:\n",
    "            return None\n",
    "\n",
    "        for word in blob.words:\n",
    "            try:\n",
    "                if language == \"en\" or translate:\n",
    "                    vector += self.en_w2v[word]\n",
    "                else:\n",
    "                    vector += self.tr_w2v[word]\n",
    "            except KeyError:\n",
    "                continue\n",
    "        vector /= len(blob.words)\n",
    "        return vector\n",
    "\n",
    "    def clean(self, text, language=\"en\", translate=True, stem=False):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').lower().decode(\"ascii\")\n",
    "        # if language == \"tr\":\n",
    "        #     if stem:\n",
    "        #         text= ' '.join([self.turkish_stemmer.stem(w) for w in text.split()])\n",
    "        blob = TextBlob(text)\n",
    "        if translate and language != \"en\":\n",
    "            try:\n",
    "                blob = blob.translate(to=\"en\")\n",
    "            except NotTranslated:\n",
    "                return \n",
    "        text = str(blob)\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = re.sub(r'[0-9]', '#', text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\"!\", \" \", text)\n",
    "        text = re.sub(r\"\\/\", \" \", text)\n",
    "        text = re.sub(r\"\\^\", \" \", text)\n",
    "        text = re.sub(r\"\\+\", \" \", text)\n",
    "        text = re.sub(r\"\\-\", \" \", text)\n",
    "        text = re.sub(r\"\\=\", \" \", text)\n",
    "        text = re.sub(r\"'\", \" \", text)\n",
    "        text = re.sub(r\":\", \" \", text)\n",
    "        text = re.sub(r\"e(\\s)?-(\\s)?mail\", \"email\", text)\n",
    "\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"cannot \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\" e g \", \" eg \", text)\n",
    "        text = re.sub(r\" b g \", \" bg \", text)\n",
    "        text = re.sub(r\" u s \", \" american \", text)\n",
    "        return TextBlob(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from AdaptiveOnlineClustering import AdaptiveOnlineClustering as aoc\n",
    "import gensim, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pickle.load(open('../datasets/tweets.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/ammar/NLP_data/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AdaptiveOnlineClustering(model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:75: UserWarning: Invalid text. Document skipped\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:53: UserWarning: An error occured during updating the cluster metrics. Last changes reveresed.\n"
     ]
    }
   ],
   "source": [
    "for t in tweets[:10]:\n",
    "    for i in t[:30]:\n",
    "#         if i.lang == \"en\":\n",
    "        a.add(i.text, i.lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
