{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, nlargest\n",
    "import warnings\n",
    "import unicodedata, re\n",
    "try:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    usesklearn = True\n",
    "except ImportError:\n",
    "    usesklearn = False\n",
    "try:\n",
    "    import networkx as nx\n",
    "except ImportError:\n",
    "    warnings.warn(\"Could not import networkx. Constructing cluster network is not available\")\n",
    "    pass\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "from textblob import TextBlob\n",
    "from textblob.exceptions import NotTranslated\n",
    "\n",
    "import gensim, pickle\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, samples=5, keep_authors_status=False):\n",
    "        self.cluster_vector = None\n",
    "        self.vectors_sim = None  # [(similarity, np.zeros(vector_size))]\n",
    "        self.vectors = list()  # [list of cluster vectors]\n",
    "        self.authors = dict()\n",
    "        self.keep_authors_status = keep_authors_status\n",
    "        self.samples = [None for i in range(samples)]\n",
    "        self._nonesamples = samples-1\n",
    "\n",
    "    def root_similarity(self, v1):\n",
    "        \"\"\"\n",
    "            similarity_to_cluster_vector\n",
    "        :param v1: np 1-D array\n",
    "        :return: similarity to the cluster vector\n",
    "        \"\"\"\n",
    "        return self.cos_sim(v1, self.cluster_vector)\n",
    "\n",
    "    def get_top_n(self, n=5):\n",
    "        \"\"\"\n",
    "        :param n: number of most similar vectors\n",
    "        :return: most similar n vectors to that cluster\n",
    "        \"\"\"\n",
    "        ## TODO return items\n",
    "        if n > len(self.vectors):\n",
    "            warnings.warn(\"n is bigger than the number of vectors in that cluster\")\n",
    "        return nlargest(min(n, len(self.vectors)), self.vectors_sim)\n",
    "\n",
    "    @staticmethod\n",
    "    def cos_sim(v1, v2):\n",
    "        if usesklearn:\n",
    "            return np.float64(cosine_similarity(np.atleast_2d(v1),np.atleast_2d(v2)))\n",
    "        else:\n",
    "            return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "    def update_cluster_vector(self):\n",
    "        self.vectors_sim = list()\n",
    "        self.cluster_vector = np.mean(np.array(self.vectors), axis=0)\n",
    "        for vector in self.vectors:\n",
    "            heappush(self.vectors_sim, (self.root_similarity(vector), vector))\n",
    "            # self.vectors_sim.append((self.root_similarity(vector), vector))\n",
    "        assert self.cluster_vector is not None\n",
    "    \n",
    "    def addtext(self, text, author):\n",
    "        if self.keep_authors_status and author:\n",
    "            self.authors.setdefault(author, list())\n",
    "            self.authors[author].append(text)\n",
    "        else:\n",
    "            self.authors.setdefault(author, 0)\n",
    "            self.authors[author] += 1\n",
    "        \n",
    "        if self._nonesamples >= 0:\n",
    "            self.samples[self._nonesamples] = text\n",
    "            self._nonesamples -= 1\n",
    "        elif np.random.choice([True, False]):\n",
    "            self.samples[np.random.randint(0, len(self.samples))] = text\n",
    "    \n",
    "    def add(self, vector, text, author):\n",
    "#         try:\n",
    "        self.vectors.append(vector)\n",
    "        self.addtext(text, author)\n",
    "        self.update_cluster_vector()\n",
    "#         except:\n",
    "#             warnings.warn(\"An error occured during updating the cluster metrics. Last changes reveresed.\")\n",
    "#             return True\n",
    "\n",
    "    def __hash__(self):\n",
    "        h = hash(str(self.cluster_vector))\n",
    "        for i in self.vectors[:min(len(self.vectors), 3)]:\n",
    "            h *= hash(str(i))\n",
    "        return h\n",
    "\n",
    "\n",
    "class AdaptiveOnlineClustering:\n",
    "\n",
    "    def __init__(self, en_w2v, tr_w2v, similarity_threshold=0.75, cluster_nsamples=5, vector_size=300):\n",
    "        self.en_w2v = en_w2v\n",
    "        self.tr_w2v = tr_w2v\n",
    "        self.vector_size = vector_size\n",
    "        self.turkish_stemmer = TurkishStemmer()\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.cluster_nsamples = cluster_nsamples\n",
    "        self.clusters = dict()\n",
    "\n",
    "    def add(self, text, language=\"en\", author=None, translate=True, stem=False):\n",
    "        vec = self.vectorize(text, language, translate=translate, stem=stem)\n",
    "        if vec is None:\n",
    "            warnings.warn(\"Invalid text. Document skipped\")\n",
    "        else:\n",
    "            self._add(vec, text, author)\n",
    "\n",
    "    def _add(self, vector, text, author):\n",
    "        highest_similarity = 0\n",
    "        assigned_cluster = None\n",
    "        for cluster in self.clusters:\n",
    "            sim = self.clusters[cluster].root_similarity(vector)\n",
    "            if sim > highest_similarity:\n",
    "                highest_similarity = sim\n",
    "                assigned_cluster = cluster\n",
    "        if highest_similarity >= self.similarity_threshold:\n",
    "            self.clusters[assigned_cluster].add(vector, text, author)\n",
    "        else:\n",
    "            new_cluster = Cluster(self.cluster_nsamples)\n",
    "            added = new_cluster.add(vector, text, author)\n",
    "            self.clusters[len(self.clusters)] = new_cluster\n",
    "        self._update_clusters()\n",
    "\n",
    "    def _update_clusters(self):\n",
    "        for cluster in self.clusters:\n",
    "            if len(self.clusters[cluster].vectors) < 1:\n",
    "                del self.clusters[cluster]\n",
    "\n",
    "    def vectorize(self, text, language, translate=True, stem=False):\n",
    "        blob = self.clean(text, language, translate=translate, stem=stem)\n",
    "        if not blob:\n",
    "            return\n",
    "        vector = np.zeros(self.vector_size)\n",
    "        if len(blob.words) < 1:\n",
    "            return None\n",
    "\n",
    "        for word in blob.words:\n",
    "            try:\n",
    "                if language == \"en\" or translate:\n",
    "                    vector += self.en_w2v[word]\n",
    "                else:\n",
    "                    vector += self.tr_w2v[word]\n",
    "            except KeyError:\n",
    "                continue\n",
    "        vector /= len(blob.words)\n",
    "        return vector\n",
    "\n",
    "    def clean(self, text, language=\"en\", translate=True, stem=False):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').lower().decode(\"ascii\")\n",
    "        # if language == \"tr\":\n",
    "        #     if stem:\n",
    "        #         text= ' '.join([self.turkish_stemmer.stem(w) for w in text.split()])\n",
    "        blob = TextBlob(text)\n",
    "        if translate and language != \"en\":\n",
    "            try:\n",
    "                blob = blob.translate(to=\"en\")\n",
    "            except NotTranslated:\n",
    "                return \n",
    "        text = str(blob)\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = re.sub(r'[0-9]', '#', text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\"!\", \" \", text)\n",
    "        text = re.sub(r\"\\/\", \" \", text)\n",
    "        text = re.sub(r\"\\^\", \" \", text)\n",
    "        text = re.sub(r\"\\+\", \" \", text)\n",
    "        text = re.sub(r\"\\-\", \" \", text)\n",
    "        text = re.sub(r\"\\=\", \" \", text)\n",
    "        text = re.sub(r\"'\", \" \", text)\n",
    "        text = re.sub(r\":\", \" \", text)\n",
    "        text = re.sub(r\"e(\\s)?-(\\s)?mail\", \"email\", text)\n",
    "\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"cannot \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\" e g \", \" eg \", text)\n",
    "        text = re.sub(r\" b g \", \" bg \", text)\n",
    "        text = re.sub(r\" u s \", \" american \", text)\n",
    "        return TextBlob(text)\n",
    "\n",
    "    def get_network_graph(self, print_prob=0.0):\n",
    "        network_dict = dict() # {(from,to): weight}\n",
    "        for c in self.clusters.values():\n",
    "            for f, t in product(c.authors, c.authors):\n",
    "                if f==t: continue\n",
    "                if (f,t) in network_dict:\n",
    "                    network_dict[(f,t)] += 1\n",
    "                elif (t,f) in network_dict:\n",
    "                    network_dict[(t,f)] += 1\n",
    "                else:\n",
    "                    network_dict[(f,t)] = 1\n",
    "        g = nx.Graph()\n",
    "        for (f,t), weight in network_dict.items():\n",
    "            if np.random.choice(a=[False, True], p=[1-print_prob, print_prob]):\n",
    "                print(f,t,weight)\n",
    "            g.add_edge(f, t, weight=weight)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pickle.load(open('../datasets/tweets.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/ammar/NLP_data/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AdaptiveOnlineClustering(model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:112: UserWarning: Invalid text. Document skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 15.009860038757324 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for author in tweets[:10]:\n",
    "    for tweet in author[:10]:\n",
    "#         if i.lang == \"en\":\n",
    "#         print(i.author.id)\n",
    "        a.add(tweet.text, tweet.lang, tweet.author.id)\n",
    "print(\"Took {} seconds\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37 clusters of 24 documents\n",
      "Cluster 0:\n",
      "Number of samples: 5\n",
      "Number of tweets: 25\n",
      "Number of authors: 9\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {765555429640470528: 2, 795688128: 1, 927954050: 6, 2290443619: 2, 2667641060: 2, 341498661: 1, 2386861894: 3, 3879123083: 4, 3068366386: 4}\n",
      "\n",
      "Cluster 1:\n",
      "Number of samples: 3\n",
      "Number of tweets: 3\n",
      "Number of authors: 2\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {919580399847858176: 2, 927954050: 1}\n",
      "\n",
      "Cluster 2:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {927954050: 1}\n",
      "\n",
      "Cluster 3:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {341498661: 1}\n",
      "\n",
      "Cluster 4:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {341498661: 1}\n",
      "\n",
      "Cluster 5:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {341498661: 1}\n",
      "\n",
      "Cluster 6:\n",
      "Number of samples: 2\n",
      "Number of tweets: 2\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {341498661: 2}\n",
      "\n",
      "Cluster 7:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {341498661: 1}\n",
      "\n",
      "Cluster 8:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {341498661: 1}\n",
      "\n",
      "Cluster 9:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 10:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 11:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 12:\n",
      "Number of samples: 5\n",
      "Number of tweets: 20\n",
      "Number of authors: 7\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {765555429640470528: 4, 795688128: 7, 3068366386: 2, 2290443619: 2, 2667641060: 1, 2386861894: 2, 3879123083: 2}\n",
      "\n",
      "Cluster 13:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 14:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 15:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2290443619: 1}\n",
      "\n",
      "Cluster 16:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2386861894: 1}\n",
      "\n",
      "Cluster 17:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2386861894: 1}\n",
      "\n",
      "Cluster 18:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2386861894: 1}\n",
      "\n",
      "Cluster 19:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2386861894: 1}\n",
      "\n",
      "Cluster 20:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2386861894: 1}\n",
      "\n",
      "Cluster 21:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {3068366386: 1}\n",
      "\n",
      "Cluster 22:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {3068366386: 1}\n",
      "\n",
      "Cluster 23:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {3068366386: 1}\n",
      "\n",
      "Cluster 24:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {3879123083: 1}\n",
      "\n",
      "Cluster 25:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {3879123083: 1}\n",
      "\n",
      "Cluster 26:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {3879123083: 1}\n",
      "\n",
      "Cluster 27:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {3879123083: 1}\n",
      "\n",
      "Cluster 28:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {765555429640470528: 1}\n",
      "\n",
      "Cluster 29:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {765555429640470528: 1}\n",
      "\n",
      "Cluster 30:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {765555429640470528: 1}\n",
      "\n",
      "Cluster 31:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {765555429640470528: 1}\n",
      "\n",
      "Cluster 32:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {919580399847858176: 1}\n",
      "\n",
      "Cluster 33:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {795688128: 1}\n",
      "\n",
      "Cluster 34:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {795688128: 1}\n",
      "\n",
      "Cluster 35:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2667641060: 1}\n",
      "\n",
      "Cluster 36:\n",
      "Number of samples: 1\n",
      "Number of tweets: 1\n",
      "Number of authors: 1\n",
      "Authors IDs > # of their tweets in that cluster: \n",
      " {2667641060: 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} clusters of 24 documents\".format(len(a.clusters)))\n",
    "for e,c in a.clusters.items():\n",
    "#     print(c.samples)\n",
    "    print(\"Cluster {}:\".format(e))\n",
    "    print(\"Number of samples:\",sum([i is not None for i in c.samples]))\n",
    "    print(\"Number of tweets:\",len(c.vectors))\n",
    "    print(\"Number of authors:\",len(c.authors))\n",
    "    print(\"Authors IDs > # of their tweets in that cluster: \\n\", c.authors)\n",
    "    print()\n",
    "#     print(c.samples[0] is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765555429640470528 2290443619 4\n",
      "2290443619 3879123083 4\n",
      "765555429640470528 795688128 4\n",
      "2667641060 341498661 2\n",
      "795688128 3068366386 4\n",
      "795688128 341498661 2\n",
      "795688128 2667641060 4\n",
      "795688128 3879123083 4\n",
      "927954050 341498661 2\n",
      "2290443619 341498661 2\n",
      "2386861894 3068366386 4\n",
      "341498661 3068366386 2\n",
      "795688128 927954050 2\n",
      "341498661 2386861894 2\n",
      "927954050 3068366386 2\n",
      "927954050 2667641060 2\n",
      "795688128 2386861894 4\n",
      "765555429640470528 3879123083 4\n",
      "765555429640470528 341498661 2\n",
      "927954050 2290443619 2\n",
      "2290443619 2667641060 4\n",
      "765555429640470528 927954050 2\n"
     ]
    }
   ],
   "source": [
    "graph = a.get_network_graph(print_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
